# -*- coding: utf-8 -*-
"""Chatbot-gemini.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mahG9jHmXEvvLzc2Z08wRRR_jt45oz-C
"""

# !pip install streamlit PyPDF2 spacy sentence-transformers scikit-learn
# !python -m spacy download es_core_news_sm
pip install --upgrade pip

import streamlit as st
import PyPDF2
import re
import spacy
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import json

st.set_page_config(page_title="Chatbot de Asistencia Académica", page_icon="🎓")

# --- 1. Configuración y Carga de Modelos ---
# Cargar el modelo de SpaCy para español
try:
    nlp = spacy.load("es_core_news_sm")
except OSError:
    st.error("Modelo 'es_core_news_sm' de SpaCy no encontrado. Ejecuta: python -m spacy download es_core_news_sm")
    st.stop()

# Cargar el modelo de Sentence Transformer para embeddings
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

embedding_model = load_embedding_model()

# --- 2. Estructura de Datos (Esquema de Datos para Ofertas Educativas) ---
# Definimos la estructura de datos para cada curso
# En un sistema más robusto, se podría usar Pydantic para validación
class Course:
    def __init__(self, familia_profesional, course_code, course_name, province,
                 municipality, institution, level, year_of_study,
                 schedule_type, is_bilingual_english, is_new_offering):
        self.familia_profesional = familia_profesional
        self.course_code = course_code
        self.course_name = course_name
        self.province = province
        self.municipality = municipality
        self.institution = institution
        self.level = level
        self.year_of_study = year_of_study
        self.schedule_type = schedule_type
        self.is_bilingual_english = is_bilingual_english
        self.is_new_offering = is_new_offering

    def to_dict(self):
        return {
            "Familia_Profesional": self.familia_profesional,
            "Course_Code": self.course_code,
            "Course_Name": self.course_name,
            "Province": self.province,
            "Municipality": self.municipality,
            "Institution": self.institution,
            "Level": self.level,
            "Year_of_Study": self.year_of_study,
            "Schedule_Type": self.schedule_type,
            "Is_Bilingual_English": self.is_bilingual_english,
            "Is_New_Offering": self.is_new_offering
        }

    def __str__(self):
        bilingual_str = " (Bilingüe: Inglés)" if self.is_bilingual_english else ""
        new_str = " (Nuevo)" if self.is_new_offering else ""
        return (f"**{self.course_name}** ({self.course_code}) - Nivel: {self.level}, "
                f"Año: {self.year_of_study}, Horario: {self.schedule_type}{bilingual_str}{new_str}\n"
                f"  Ubicación: {self.institution}, {self.municipality}, {self.province}\n"
                f"  Familia Profesional: {self.familia_profesional}")

# --- 3. Extracción y Estructuración de Datos del PDF (Conceptual y con Datos de Ejemplo) ---

def parse_pdf(pdf_path):
    """
    Función conceptual para analizar el PDF y extraer la información estructurada.
    Esta es una implementación simplificada y requeriría un desarrollo robusto
    con expresiones regulares y lógica de estado para manejar la complejidad
    del documento real.
    """
    # Fix: Initialize extracted_data as an empty list
    extracted_data = []
    current_family = None
    current_level = None

    try:
        # Make sure you have PyPDF2 and re imported if you haven't already
        import PyPDF2
        import re

        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            for page_num in range(len(reader.pages)):
                page_text = reader.pages[page_num].extract_text()
                lines = page_text.split('\n')

                for i, line in enumerate(lines):
                    line = line.strip()

                    # Identificar Familia Profesional
                    family_match = re.match(r'^(ACTIVIDADES FÍSICAS Y DEPORTIVAS|ADMINISTRACIÓN Y GESTIÓN|AGRARIA|ARTES GRÁFICAS|COMERCIO Y MÁRKETING|EDIFICACIÓN Y OBRA CIVIL|ELECTRICIDAD Y ELECTRÓNICA|ENERGÍA Y AGUA|FABRICACIÓN MECÁNICA|HOSTELERÍA Y TURISMO|IMAGEN PERSONAL|IMAGEN Y SONIDO|INDUSTRIAS ALIMENTARIAS|INFORMÁTICA Y COMUNICACIONES|INSTALACIÓN Y MANTENIMIENTO|MADERA,MUEBLE Y CORCHO|QUIMICA|SANIDAD|SEGURIDAD Y MEDIO AMBIENTE|SERVICIOS SOCIOCULTURALES Y A LA COMUNIDAD|TEXTIL,CONFECCION Y PIEL|TRANSPORTE Y MANTENIMIENTO DE VEHICULOS)$', line)
                    if family_match:
                        current_family = family_match.group(1)
                        current_level = None # Reset level when family changes
                        continue

                    # Identificar Nivel
                    level_match = re.match(r'^(C\.P\.N\.[1-3]|Gr\.C N\.[1-3]|GrBásico|GrMedio|GrSuperior|CursoEspec)$', line)
                    if level_match:
                        current_level = level_match.group(1)
                        continue

                    # Intentar extraer información del curso si tenemos familia y nivel
                    if current_family and current_level:
                        course_match = re.match(r'^\((.*?)\)\s*(.*)$', line)
                        if course_match:
                            course_code = course_match.group(1).strip()
                            course_name = course_match.group(2).strip()

                            # Buscar las siguientes líneas para ubicación y detalles
                            province = None
                            municipality = None
                            institution = None
                            year_of_study = None
                            schedule_type = None
                            is_bilingual_english = False
                            is_new_offering = False

                            # Lógica para buscar las siguientes 1-3 líneas para detalles
                            for j in range(i + 1, min(i + 4, len(lines))):
                                detail_line = lines[j].strip()

                                # Provincia
                                if not province and re.match(r'^(BADAJOZ|CÁCERES)$', detail_line):
                                    province = detail_line
                                    continue

                                # Institución, Municipio, Año, Horario, Bilingüe, Nuevo
                                # Esta regex es compleja y necesitaría ser muy robusta para el PDF real
                                # Aquí se simplifica para el ejemplo
                                location_schedule_match = re.match(r'^(.*?)\s*-\s*(.*?)\s*-\s*(.*?)\s*(Diurno|Vespertino)?\s*(Bilingüe:IN)?\s*(Nuevo)?$', detail_line)
                                if location_schedule_match:
                                    # Esto es una simplificación, el parsing real sería más granular
                                    # y manejaría casos como "Centro acreditado en Calidad"
                                    full_location_str = location_schedule_match.group(1).strip()
                                    year_schedule_str = location_schedule_match.group(3).strip()

                                    # Intentar extraer municipio e institución de full_location_str
                                    # Esto es muy simplificado y puede fallar en casos complejos
                                    parts = full_location_str.split(' - ')
                                    if len(parts) >= 2:
                                        # Fix: Correct indexing for municipality and institution
                                        municipality = parts[0].strip()
                                        institution = parts[1].strip()
                                    else:
                                        institution = full_location_str # Fallback

                                    if '1°C' in year_schedule_str:
                                        year_of_study = '1°C'
                                    elif '2°C' in year_schedule_str:
                                        year_of_study = '2°C'

                                    if 'Diurno' in detail_line:
                                        schedule_type = 'Diurno'
                                    elif 'Vespertino' in detail_line:
                                        schedule_type = 'Vespertino'

                                    if 'Bilingüe:IN' in detail_line:
                                        is_bilingual_english = True
                                    if 'Nuevo' in detail_line:
                                        is_new_offering = True
                                    break # Asumimos que encontramos los detalles del curso

                            # Ensure Course class is defined or handle its absence
                            # For now, I'll comment this out as Course class is not provided in your snippet
                            # if all([province, municipality, institution, year_of_study, schedule_type]):
                            #     course_obj = Course(
                            #         familia_profesional=current_family,
                            #         course_code=course_code,
                            #         course_name=course_name,
                            #         province=province,
                            #         municipality=municipality,
                            #         institution=institution,
                            #         level=current_level,
                            #         year_of_study=year_of_study,
                            #         schedule_type=schedule_type,
                            #         is_bilingual_english=is_bilingual_english,
                            #         is_new_offering=is_new_offering
                            #     )
                            #     extracted_data.append(course_obj.to_dict())

                            # For testing purposes without the Course class, append as a dictionary
                            if all([province, municipality, institution, year_of_study, schedule_type]):
                                extracted_data.append({
                                    "familia_profesional": current_family,
                                    "course_code": course_code,
                                    "course_name": course_name,
                                    "province": province,
                                    "municipality": municipality,
                                    "institution": institution,
                                    "level": current_level,
                                    "year_of_study": year_of_study,
                                    "schedule_type": schedule_type,
                                    "is_bilingual_english": is_bilingual_english,
                                    "is_new_offering": is_new_offering
                                })

    except Exception as e:
        # Assuming st is a streamlit object
        # import streamlit as st # Uncomment if using streamlit
        # st.error(f"Error al procesar el PDF: {e}")
        # st.info("La extracción de datos de PDFs semiestructurados es compleja. Se recomienda un parser dedicado.")
        print(f"Error al procesar el PDF: {e}") # Using print for console output
        print("La extracción de datos de PDFs semiestructurados es compleja. Se recomienda un parser dedicado.")


    return extracted_data

# Datos de ejemplo para simular la salida del parser de PDF
# Estos datos son una representación simplificada y parcial del PDF real [1]
sample_courses_data = [
    {
        "familia_profesional": "INFORMÁTICA Y COMUNICACIONES",
        "level": "GrMedio",
        "course_code": "IFC101",
        "course_name": "Sistemas Microinformáticos y Redes",
        "province": "BADAJOZ",
        "municipality": "Badajoz",
        "institution": "IES TIERRAS DE BARROS",
        "year_of_study": "1°C",
        "schedule_type": "Diurno",
        "is_bilingual_english": True,
        "is_new_offering": False
    },
    {
        "familia_profesional": "INFORMÁTICA Y COMUNICACIONES",
        "level": "GrSuperior",
        "course_code": "IFC202",
        "course_name": "Desarrollo de Aplicaciones Multiplataforma",
        "province": "CÁCERES",
        "municipality": "Cáceres",
        "institution": "IES AL-QÁZERES",
        "year_of_study": "1°C",
        "schedule_type": "Diurno",
        "is_bilingual_english": False,
        "is_new_offering": True
    },
    {
        "familia_profesional": "HOSTELERÍA Y TURISMO",
        "level": "GrMedio",
        "course_code": "HOT303",
        "course_name": "Servicios en Restauración",
        "province": "BADAJOZ",
        "municipality": "Mérida",
        "institution": "IES SANTA EULALIA",
        "year_of_study": "2°C",
        "schedule_type": "Vespertino",
        "is_bilingual_english": False,
        "is_new_offering": False
    },
    {
        "familia_profesional": "SANIDAD",
        "level": "GrSuperior",
        "course_code": "SAN404",
        "course_name": "Laboratorio Clínico y Biomédico",
        "province": "CÁCERES",
        "municipality": "Plasencia",
        "institution": "IES VALLE DEL JERTE",
        "year_of_study": "1°C",
        "schedule_type": "Diurno",
        "is_bilingual_english": True,
        "is_new_offering": False
    }
]

# --- 4. Preprocesamiento de Texto (NLP_PREPROCESSOR) ---
def preprocess_text(text):
    """
    Realiza tokenización, limpieza y lematización del texto.
    Preserva códigos de curso y términos específicos.
    """
    # Convertir a minúsculas
    text = text.lower()
    # Eliminar URLs (si las hubiera, aunque el PDF no las tiene explícitamente en el contenido)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    # Eliminar caracteres especiales, pero mantener números y letras
    text = re.sub(r'[^a-záéíóúüñ\s\d\(\)-]', '', text) # Mantener paréntesis y guiones para códigos
    # Eliminar múltiples espacios
    text = re.sub(r'\s+', ' ', text).strip()

    doc = nlp(text)
    lemmas = []
    for token in doc:
        # Conservar códigos de curso y niveles específicos como tokens únicos
        if re.match(r'^\(?[a-z]{2,4}\d{1,2}-\d{1,2}\)?$', token.text) or \
           re.match(r'^(c\.p\.n\.[1-3]|gr\.c n\.[1-3]|grbásico|grmedio|grsuperior|cursoespec)$', token.text) or \
           re.match(r'^\d+c$', token.text) or \
           token.text in ["diurno", "vespertino", "bilingüe:in", "nuevo", "extremadura", "badajoz", "cáceres"]:
            lemmas.append(token.text)
        elif not token.is_stop and not token.is_punct and not token.is_space:
            lemmas.append(token.lemma_)
    return " ".join(lemmas)

# --- 5. Representación del Conocimiento (VECTOR_DB) ---
class VectorDatabase:
    def __init__(self, courses, embedding_model):
        self.courses = courses
        self.embedding_model = embedding_model
        self.course_texts = []
        self.course_embeddings = []
        self._build_index()

    def _build_index(self):
        """
        Crea los textos para indexar y sus embeddings.
        Combina varios campos para una representación rica.
        """
        for course in self.courses:
            # Crear un texto descriptivo para cada curso para el embedding
            text_to_embed = (
                # Corrected key names to match the dictionary structure
                f"{course['familia_profesional']} "
                f"{course['course_name']} {course['course_code']} "
                f"{course['level']} {course['institution']} "
                f"{course['municipality']} {course['province']} "
                # The next line "f"{course}"" seems incorrect as it will try to
                # convert the entire dictionary object to a string, which is
                # usually not helpful for embeddings. Let's remove or adjust it.
                # For now, removing it.
                # f"{course}"
            )
            # The check 'if course:' will always be true if course is a non-empty dict.
            # It should check a specific key related to bilingualism.
            # Assuming 'is_bilingual_english' is the correct key based on sample data
            if course.get('is_bilingual_english', False): # Use .get() with a default to be safe
                text_to_embed += " bilingüe inglés"
            if course['is_new_offering']:
                text_to_embed += " nuevo"

            self.course_texts.append(preprocess_text(text_to_embed))

        # Generar embeddings en lotes para eficiencia
        self.course_embeddings = self.embedding_model.encode(self.course_texts, show_progress_bar=False)

    def search(self, query, top_k=5):
        """
        Busca los cursos más relevantes para una consulta dada.
        """
        processed_query = preprocess_text(query)
        query_embedding = self.embedding_model.encode([processed_query])

        # Ensure query_embedding is 2D
        query_embedding = np.array(query_embedding)
        if query_embedding.ndim == 3:
            query_embedding = query_embedding.squeeze()
        if query_embedding.ndim == 1:
            query_embedding = query_embedding.reshape(1, -1)

        # Ensure course_embeddings is 2D
        self.course_embeddings = np.array(self.course_embeddings)
        if self.course_embeddings.ndim == 3:
            self.course_embeddings = self.course_embeddings.squeeze()

        # Compute cosine similarity
        similarities = cosine_similarity(query_embedding, self.course_embeddings)
        #similarities = cosine_similarity([query_embedding], self.course_embeddings)

        # Ordenar por similitud y obtener los índices de los top_k
        top_indices = np.argsort(similarities)[0][::-1][:top_k] # Corrected indexing for similarities

        results = []
        for idx in top_indices:
            results.append({
                "course": self.courses[idx],
                "similarity": similarities[0][idx] # Corrected indexing for similarities
            })
        return results

# Inicializar la base de datos vectorial con los datos de ejemplo
vector_db = VectorDatabase(sample_courses_data, embedding_model)

# --- 6. Sistema RAG (Generación Aumentada por Recuperación) ---
class RAGSystem:
    def __init__(self, vector_db):
        self.vector_db = vector_db

    def retrieve_information(self, query):
        """
        Recupera los documentos más relevantes de la base de conocimiento.
        """
        # Aquí se podría añadir lógica para filtrar por metadatos antes de la búsqueda semántica
        # Por ejemplo, si la consulta incluye "provincia: Cáceres", filtrar primero por provincia.

        # Para este ejemplo, solo usamos la búsqueda semántica
        relevant_courses = self.vector_db.search(query, top_k=5)

        context = []
        for res in relevant_courses:
            course_obj = Course(**res['course'])
            context.append(str(course_obj))

        return "\n\n".join(context)

    def generate_response_with_llm(self, user_query, retrieved_context):
        """
        Genera una respuesta utilizando un LLM y el contexto recuperado.

        NOTA: Esta función es un placeholder. Para una implementación real,
        necesitarías integrar un LLM (por ejemplo, OpenAI, Gemini, Hugging Face local)
        y usar tu clave API si es necesario.
        """
        if not retrieved_context:
            return "Lo siento, no pude encontrar información relevante en la oferta formativa para tu consulta."

        # Prompt engineering para guiar al LLM
        prompt = (
            "Eres un asistente experto en la oferta formativa de la Junta de Extremadura para el año 2025/2026. "
            "Tu objetivo es proporcionar información precisa y concisa basada *exclusivamente* en el contexto proporcionado. "
            "Si la información no está en el contexto, indica que no puedes responder a esa pregunta. "
            "No alucines ni inventes información.\n\n"
            "Contexto de la oferta formativa:\n"
            f"{retrieved_context}\n\n"
            f"Pregunta del usuario: {user_query}\n\n"
            "Respuesta:"
        )

        # --- Placeholder para la llamada al LLM ---
        # Ejemplo de cómo se haría con un LLM real (requiere configuración de API key)
        # from openai import OpenAI
        # client = OpenAI(api_key="TU_API_KEY")
        # response = client.chat.completions.create(
        #     model="gpt-3.5-turbo", # O el modelo que prefieras
        #     messages=[{"role": "user", "content": prompt}],
        #     temperature=0.1, # Baja temperatura para respuestas más deterministas
        # ).choices.message.content
        # return response

        # Simulación de respuesta del LLM basada en el contexto
        # En un LLM real, la respuesta sería más elaborada y resumiría el contexto.
        # Aquí, simplemente devolvemos el contexto si es relevante.

        # Lógica simple para simular una respuesta basada en el contexto
        if "curso" in user_query.lower() or "formación" in user_query.lower() or "programa" in user_query.lower():
            return f"Según la oferta formativa, aquí tienes la información relevante:\n\n{retrieved_context}\n\n¿Hay algo más en lo que pueda ayudarte?"
        elif "hola" in user_query.lower() or "saludo" in user_query.lower():
            return "¡Hola! Soy tu asistente experto en la oferta formativa de la Junta de Extremadura. ¿En qué puedo ayudarte hoy?"
        else:
            return f"He encontrado la siguiente información relacionada con tu consulta:\n\n{retrieved_context}\n\nPor favor, especifica si necesitas más detalles sobre algún curso o familia profesional."


rag_system = RAGSystem(vector_db)

# --- 7. Interfaz de Usuario con Streamlit ---
#st.set_page_config(page_title="Chatbot de Asistencia Académica", page_icon="🎓")

st.title("🎓 Chatbot de Asistencia Académica")
st.info("Soy un asistente experto en la oferta formativa de la Junta de Extremadura para el año académico 2025/2026. Puedes preguntarme sobre cursos, niveles, ubicaciones o familias profesionales.")

# Inicializar historial de chat en el estado de sesión
if "messages" not in st.session_state:
    st.session_state.messages = []

# Mostrar mensajes del historial de chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Entrada de usuario
if prompt := st.chat_input("Hazme una pregunta sobre la oferta formativa..."):
    # Añadir mensaje del usuario al historial
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Buscando información..."):
            # 1. Recuperar información relevante
            context = rag_system.retrieve_information(prompt)

            # 2. Generar respuesta con el LLM (placeholder)
            response = rag_system.generate_response_with_llm(prompt, context)

            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})

# Opcional: Botón para limpiar el historial de chat
if st.sidebar.button("Limpiar Chat"):
    st.session_state.messages = []
    st.rerun()

# Opcional: Cargar PDF (solo para demostración del parser conceptual)
# st.sidebar.header("Cargar PDF (Opcional)")
# uploaded_file = st.sidebar.file_uploader("Sube el archivo 'Oferta formativa. Mod. Presencial Completa 25/26.pdf'", type="pdf")
# if uploaded_file is not None:
#     st.sidebar.write("Archivo cargado. Procesando (esto puede tardar)...")
#     # Guardar el archivo temporalmente para PyPDF2
#     with open("temp_oferta.pdf", "wb") as f:
#         f.write(uploaded_file.getbuffer())
#
#     parsed_data = parse_pdf("temp_oferta.pdf")
#     if parsed_data:
#         st.sidebar.success(f"Se extrajeron {len(parsed_data)} entradas de cursos del PDF.")
#         # Aquí podrías actualizar vector_db con los datos del PDF si el parsing fuera robusto
#         # vector_db = VectorDatabase(parsed_data, embedding_model)
#         # rag_system = RAGSystem(vector_db)
#         st.sidebar.json(parsed_data[:5]) # Mostrar las primeras 5 entradas
#     else:
#         st.sidebar.warning("No se pudo extraer información estructurada del PDF con el parser actual.")
